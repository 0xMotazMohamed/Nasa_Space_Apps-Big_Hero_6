{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13255003,"sourceType":"datasetVersion","datasetId":8399378},{"sourceId":13255485,"sourceType":"datasetVersion","datasetId":8399336}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset,DataLoader\nfrom sklearn.preprocessing import MinMaxScaler\nimport torch.cuda.amp as amp\nimport torch.nn.functional as F\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom pathlib import Path\nimport os","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T04:25:47.695529Z","iopub.execute_input":"2025-10-04T04:25:47.696413Z","iopub.status.idle":"2025-10-04T04:25:47.701178Z","shell.execute_reply.started":"2025-10-04T04:25:47.696387Z","shell.execute_reply":"2025-10-04T04:25:47.700480Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"#df = pd.read_csv('/kaggle/input/no2-data/merged_no2_pixels.csv')\ndf = pd.read_csv('/kaggle/input/tempo-no2-data/merged_hcho_pixels.csv')\n#df = pd.read_csv('/kaggle/input/tempo-no2-data/merged_o3_pixels.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T04:25:47.702373Z","iopub.execute_input":"2025-10-04T04:25:47.702786Z","iopub.status.idle":"2025-10-04T04:26:03.267391Z","shell.execute_reply.started":"2025-10-04T04:25:47.702769Z","shell.execute_reply":"2025-10-04T04:26:03.266580Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"'''tmp=df\ntmp ['day']= df['datetime']-df['datetime'].iloc[0]\ntmp = tmp[['day', 'datetime']]\ntmp'''\nLAST_DATE=pd.to_datetime('2025-10-03')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T04:26:03.268294Z","iopub.execute_input":"2025-10-04T04:26:03.268590Z","iopub.status.idle":"2025-10-04T04:26:03.273306Z","shell.execute_reply.started":"2025-10-04T04:26:03.268562Z","shell.execute_reply":"2025-10-04T04:26:03.272484Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"### Clip outliers per pixel (IQR)\n","metadata":{}},{"cell_type":"code","source":"df['datetime'] = pd.to_datetime(df['datetime'])\ndf = df.sort_values('datetime').reset_index(drop=True)\nLAST_DATE=pd.to_datetime(df.iloc[-1]['datetime'])\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T04:26:03.275356Z","iopub.execute_input":"2025-10-04T04:26:03.275627Z","iopub.status.idle":"2025-10-04T04:26:03.493255Z","shell.execute_reply.started":"2025-10-04T04:26:03.275609Z","shell.execute_reply":"2025-10-04T04:26:03.492512Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"      datetime    0    1    2    3    4    5    6    7    8  ...  36570  \\\n0   2023-08-02  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n1   2023-08-04  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n2   2023-08-05  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n3   2023-08-06  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n4   2023-08-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n..         ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...    ...   \n756 2025-09-29  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n757 2025-09-30  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n758 2025-10-01  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n759 2025-10-02  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n760 2025-10-03  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n\n     36571  36572  36573  36574  36575  36576  36577  36578  36579  \n0      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n1      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n2      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n3      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n4      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n..     ...    ...    ...    ...    ...    ...    ...    ...    ...  \n756    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n757    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n758    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n759    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n760    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n\n[761 rows x 36581 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>datetime</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>...</th>\n      <th>36570</th>\n      <th>36571</th>\n      <th>36572</th>\n      <th>36573</th>\n      <th>36574</th>\n      <th>36575</th>\n      <th>36576</th>\n      <th>36577</th>\n      <th>36578</th>\n      <th>36579</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2023-08-02</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2023-08-04</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2023-08-05</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2023-08-06</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2023-08-07</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>756</th>\n      <td>2025-09-29</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>757</th>\n      <td>2025-09-30</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>758</th>\n      <td>2025-10-01</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>759</th>\n      <td>2025-10-02</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>760</th>\n      <td>2025-10-03</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>761 rows × 36581 columns</p>\n</div>"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"all_days = pd.date_range(df['datetime'].min().date(), df['datetime'].max().date(), freq=\"D\")\ndf_full = df.set_index('datetime').reindex(all_days).reset_index()\ndf_full = df_full.rename(columns={'index': 'datetime'})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T04:26:03.493947Z","iopub.execute_input":"2025-10-04T04:26:03.494140Z","iopub.status.idle":"2025-10-04T04:26:03.832742Z","shell.execute_reply.started":"2025-10-04T04:26:03.494126Z","shell.execute_reply":"2025-10-04T04:26:03.832135Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"pixel_data = df.drop(columns=['datetime']).values\nQ1 = np.percentile(pixel_data, 25, axis=0)\nQ3 = np.percentile(pixel_data, 75, axis=0)\nIQR = Q3 - Q1\nlower = Q1 - 1.5 * IQR\nupper = Q3 + 1.5 * IQR\npixel_data_capped = np.clip(pixel_data, lower, upper)\npixel_data_capped = np.clip(pixel_data_capped, 0, None)\n\ndf_capped = pd.DataFrame(pixel_data_capped, columns=df_full.columns[1:])\ndf_capped.insert(0, 'datetime', df_full['datetime'])\ndf=df_capped","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T04:26:03.833484Z","iopub.execute_input":"2025-10-04T04:26:03.833691Z","iopub.status.idle":"2025-10-04T04:26:05.071931Z","shell.execute_reply.started":"2025-10-04T04:26:03.833675Z","shell.execute_reply":"2025-10-04T04:26:05.071015Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"\n# Interpolate missing values per pixel\ndf.iloc[:,1:] = df.iloc[:,1:].interpolate(limit_direction='both')\nprint(\"After filling gaps:\", df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T04:26:05.072865Z","iopub.execute_input":"2025-10-04T04:26:05.073072Z","iopub.status.idle":"2025-10-04T04:26:07.906593Z","shell.execute_reply.started":"2025-10-04T04:26:05.073056Z","shell.execute_reply":"2025-10-04T04:26:07.905728Z"}},"outputs":[{"name":"stdout","text":"After filling gaps: (761, 36581)\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"### Scaling","metadata":{}},{"cell_type":"code","source":"raw_scaler = MinMaxScaler()\nraw_scaler.fit(df.drop(columns=['datetime']))\n\nscaler = MinMaxScaler()\npixel_data_scaled = scaler.fit_transform(df.drop(columns=['datetime']))\n\ndf_scaled = pd.DataFrame(pixel_data_scaled, columns=df.columns[1:])\ndf_scaled.insert(0, 'datetime', df['datetime'])\n\nprint(\"Scaling done, shape:\", df_scaled.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T04:26:07.907488Z","iopub.execute_input":"2025-10-04T04:26:07.907770Z","iopub.status.idle":"2025-10-04T04:26:09.660183Z","shell.execute_reply.started":"2025-10-04T04:26:07.907750Z","shell.execute_reply":"2025-10-04T04:26:09.659360Z"}},"outputs":[{"name":"stdout","text":"Scaling done, shape: (761, 36581)\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"H, W = 118, 310  # Original dimensions\n#H, W = 122, 310  # Original dimensions\n\nTARGET_H, TARGET_W = 256, 256  # Target dimensions for pre-trained model\n\n# Working directories\nWORK_DIR = Path('/kaggle/working/unet_forecast')\ndays_dir = WORK_DIR / 'days_npy'\nmeta_path = WORK_DIR / 'days_metadata.csv'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T04:26:09.661649Z","iopub.execute_input":"2025-10-04T04:26:09.661873Z","iopub.status.idle":"2025-10-04T04:26:09.666138Z","shell.execute_reply.started":"2025-10-04T04:26:09.661856Z","shell.execute_reply":"2025-10-04T04:26:09.665326Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"### Downsampling Stage","metadata":{}},{"cell_type":"code","source":"if days_dir.exists():\n    print('Cleaning existing days directory...')\n    for f in days_dir.glob('*.npy'):\n        f.unlink()   # delete existing .npy files\nelse:\n    days_dir.mkdir(parents=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T04:26:09.668819Z","iopub.execute_input":"2025-10-04T04:26:09.669070Z","iopub.status.idle":"2025-10-04T04:26:09.684049Z","shell.execute_reply.started":"2025-10-04T04:26:09.669053Z","shell.execute_reply":"2025-10-04T04:26:09.683303Z"}},"outputs":[{"name":"stdout","text":"Cleaning existing days directory...\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"dates = df_scaled['datetime']             # pandas Series of dates\ndata = df_scaled.drop(columns=['datetime']).values  # numpy array of pixel values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T04:26:09.684907Z","iopub.execute_input":"2025-10-04T04:26:09.685138Z","iopub.status.idle":"2025-10-04T04:26:09.779225Z","shell.execute_reply.started":"2025-10-04T04:26:09.685120Z","shell.execute_reply":"2025-10-04T04:26:09.778399Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"Compute how much padding is needed on each side so that (H, W) \n\nU-Net is divisible by 16 ","metadata":{}},{"cell_type":"code","source":"# No padding needed since we're resizing to 256x256\nprint(f\"Resizing images from {H}x{W} to {TARGET_H}x{TARGET_W}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T04:26:09.780179Z","iopub.execute_input":"2025-10-04T04:26:09.780502Z","iopub.status.idle":"2025-10-04T04:26:09.784767Z","shell.execute_reply.started":"2025-10-04T04:26:09.780472Z","shell.execute_reply":"2025-10-04T04:26:09.784000Z"}},"outputs":[{"name":"stdout","text":"Resizing images from 118x310 to 256x256\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"import cv2\n\nmeta_rows = []\nfor i, d in enumerate(dates):\n    arr = data[i].reshape(H, W)\n    # Resize to 256x256 using OpenCV\n    arr_resized = cv2.resize(arr, (TARGET_W, TARGET_H), interpolation=cv2.INTER_LINEAR)\n    fpath = days_dir/f\"day_{i:04d}.npy\"\n    np.save(fpath, arr_resized.astype(np.float32))\n    meta_rows.append([d, fpath.as_posix(), H, W])  # Store original dimensions\n\nmeta_df = pd.DataFrame(meta_rows, columns=['datetime','file','original_h','original_w'])\n\nmeta_df.to_csv(meta_path, index=False)\nmeta_df\nprint(f\"Saved {len(meta_df)} resized days to {days_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T04:26:09.785615Z","iopub.execute_input":"2025-10-04T04:26:09.785875Z","iopub.status.idle":"2025-10-04T04:26:10.404823Z","shell.execute_reply.started":"2025-10-04T04:26:09.785857Z","shell.execute_reply":"2025-10-04T04:26:10.404128Z"}},"outputs":[{"name":"stdout","text":"Saved 761 resized days to /kaggle/working/unet_forecast/days_npy\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"### Dataset Sequence","metadata":{}},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"code","source":"class DaySequenceDataset(Dataset):\n    def __init__(self, meta_df, lookback_days):\n        self.meta_df = meta_df.reset_index(drop=True)\n        self.lookback_days = lookback_days\n    def __len__(self):\n        return len(self.meta_df) - self.lookback_days\n    def __getitem__(self, idx):\n        files = self.meta_df['file'].iloc[idx:idx+self.lookback_days].tolist()\n        x = np.stack([np.load(f) for f in files], axis=0)  # Shape: (LOOKBACK_DAYS, 256, 256)\n        y = np.load(self.meta_df['file'].iloc[idx+self.lookback_days])  # Shape: (256, 256)\n        return torch.from_numpy(x).float(), torch.from_numpy(y).float()\n\nLOOKBACK_DAYS = 7\ntrain_size = int(len(meta_df)*0.90)\nval_size = len(meta_df) - (LOOKBACK_DAYS+3)\n\ntrain_meta = meta_df.iloc[:train_size]\nval_meta = meta_df.iloc[train_size:]\ntest_meta =  meta_df.iloc[val_size:]\n\ntrain_dataset = DaySequenceDataset(train_meta, LOOKBACK_DAYS)\nval_dataset = DaySequenceDataset(val_meta, LOOKBACK_DAYS)\ntest_dataset = DaySequenceDataset(test_meta, LOOKBACK_DAYS)\n\n\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=3, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=3, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=3, pin_memory=True)\n\nprint(f\"Train samples: {len(train_dataset)} Val samples: {len(val_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T04:26:10.405670Z","iopub.execute_input":"2025-10-04T04:26:10.405886Z","iopub.status.idle":"2025-10-04T04:26:10.416478Z","shell.execute_reply.started":"2025-10-04T04:26:10.405868Z","shell.execute_reply":"2025-10-04T04:26:10.415678Z"}},"outputs":[{"name":"stdout","text":"Train samples: 677 Val samples: 70\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"# Load pre-trained U-Net model from torch.hub\n# Note: This model expects 3-channel input, so we'll need to adapt our data\nprint(\"Loading pre-trained U-Net model...\")\nmodel_hub = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n    in_channels=3, out_channels=1, init_features=32, pretrained=True)\n\n# Create a wrapper to handle single-channel to 3-channel conversion\nclass UNetWrapper(nn.Module):\n    def __init__(self, pretrained_model, input_channels=7):\n        super().__init__()\n        self.pretrained_model = pretrained_model\n        # Add a 1x1 conv to convert from LOOKBACK_DAYS channels to 3 channels\n        self.channel_adapter = nn.Conv2d(input_channels, 3, kernel_size=1)\n        \n    def forward(self, x):\n        # Convert from LOOKBACK_DAYS channels to 3 channels\n        x_adapted = self.channel_adapter(x)\n        # Pass through pre-trained U-Net\n        return self.pretrained_model(x_adapted)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T04:26:10.417312Z","iopub.execute_input":"2025-10-04T04:26:10.417655Z","iopub.status.idle":"2025-10-04T04:26:14.198741Z","shell.execute_reply.started":"2025-10-04T04:26:10.417618Z","shell.execute_reply":"2025-10-04T04:26:14.198142Z"}},"outputs":[{"name":"stdout","text":"Loading pre-trained U-Net model...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/hub.py:330: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n  warnings.warn(\nDownloading: \"https://github.com/mateuszbuda/brain-segmentation-pytorch/zipball/master\" to /root/.cache/torch/hub/master.zip\nDownloading: \"https://github.com/mateuszbuda/brain-segmentation-pytorch/releases/download/v1.0/unet-e012d006.pt\" to /root/.cache/torch/hub/checkpoints/unet-e012d006.pt\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = UNetWrapper(model_hub, input_channels=LOOKBACK_DAYS).to(DEVICE)\nprint('Model params:', sum(p.numel() for p in model.parameters()))\nprint('Pre-trained U-Net loaded successfully!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T04:26:14.199598Z","iopub.execute_input":"2025-10-04T04:26:14.199908Z","iopub.status.idle":"2025-10-04T04:26:14.533666Z","shell.execute_reply.started":"2025-10-04T04:26:14.199867Z","shell.execute_reply":"2025-10-04T04:26:14.532952Z"}},"outputs":[{"name":"stdout","text":"Model params: 7763065\nPre-trained U-Net loaded successfully!\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"code","source":"# Replace the existing training cell with this enhanced version\n\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=7e-6)\nscaler_amp = amp.GradScaler()\n\nEPOCHS = 80   \npatience = 5            \nbest_val_loss = float('inf')\npatience_counter = 0\n\nprint(\"Starting initial training phase...\")\nprint(\"=\"*50)\n\nfor epoch in range(EPOCHS):\n    # Training\n    model.train()\n    running_loss = 0.0\n    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} (train)\")\n    for xb, yb in loop:\n        xb = xb.to(DEVICE, non_blocking=True)\n        yb = yb.to(DEVICE, non_blocking=True)\n        optimizer.zero_grad()\n        with amp.autocast():\n            preds = model(xb)\n            loss = criterion(preds, yb.unsqueeze(1))\n        scaler_amp.scale(loss).backward()\n        scaler_amp.step(optimizer)\n        scaler_amp.update()\n        running_loss += loss.item()\n        loop.set_postfix(loss=loss.item())\n    train_loss = running_loss / len(train_loader)\n\n    # Validation\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for xb, yb in val_loader:\n            xb = xb.to(DEVICE, non_blocking=True)\n            yb = yb.to(DEVICE, non_blocking=True)\n            with amp.autocast():\n                preds = model(xb)\n                loss = criterion(preds, yb.unsqueeze(1))\n            val_loss += loss.item()\n    val_loss /= len(val_loader)\n\n    print(f\"Epoch {epoch+1}: train loss {train_loss:.4f}, val loss {val_loss:.4f}\")\n\n    # Early stopping check \n    if val_loss < best_val_loss - 1e-6:  \n        best_val_loss = val_loss\n        patience_counter = 0\n        torch.save(model.state_dict(), '/kaggle/working/best_model.pth')\n    else:\n        patience_counter += 1\n        print(f\"No improvement for {patience_counter} epoch(s)\")\n        if patience_counter >= patience:\n            print(\"Early stopping triggered.\")\n            break\n\nprint(f\"\\nInitial training completed. Best validation loss: {best_val_loss:.6f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T04:26:14.534361Z","iopub.execute_input":"2025-10-04T04:26:14.534604Z"}},"outputs":[{"name":"stdout","text":"Starting initial training phase...\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/2090717526.py:5: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler_amp = amp.GradScaler()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/80 (train):   0%|          | 0/170 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c4cb667446b4a8f98a0987d3d3e899c"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/2090717526.py:24: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with amp.autocast():\n/tmp/ipykernel_36/2090717526.py:41: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: train loss 0.0537, val loss 0.0389\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/80 (train):   0%|          | 0/170 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0092cd468ec42c3bd314e8d21d4fd40"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"# Fine-tuning Phase on Validation Data\nprint(\"\\n\" + \"=\"*50)\nprint(\"Starting fine-tuning phase on validation data...\")\nprint(\"=\"*50)\n\n# Load the best model from initial training\nmodel.load_state_dict(torch.load('/kaggle/working/best_model.pth', map_location=DEVICE))\n\n# Create fine-tuning optimizer with lower learning rate\nfinetune_lr = 2e-6  # Lower learning rate for fine-tuning\nfinetune_optimizer = optim.Adam(model.parameters(), lr=finetune_lr)\nfinetune_scaler = amp.GradScaler()\n\n# Fine-tuning parameters\nFINETUNE_EPOCHS = 10\nfinetune_patience = 3\nbest_finetune_loss = float('inf')\nfinetune_patience_counter = 0\n\n# Track losses for comparison\ninitial_val_loss = best_val_loss\nfinetune_losses = []\n\nprint(f\"Fine-tuning with learning rate: {finetune_lr}\")\nprint(f\"Initial validation loss: {initial_val_loss:.6f}\")\n\nfor epoch in range(FINETUNE_EPOCHS):\n    model.train()\n    running_finetune_loss = 0.0\n    \n    # Train on validation data (fine-tuning)\n    loop = tqdm(val_loader, desc=f\"Fine-tune Epoch {epoch+1}/{FINETUNE_EPOCHS}\")\n    for xb, yb in loop:\n        xb = xb.to(DEVICE, non_blocking=True)\n        yb = yb.to(DEVICE, non_blocking=True)\n        \n        finetune_optimizer.zero_grad()\n        with amp.autocast():\n            preds = model(xb)\n            loss = criterion(preds, yb.unsqueeze(1))\n        \n        finetune_scaler.scale(loss).backward()\n        finetune_scaler.step(finetune_optimizer)\n        finetune_scaler.update()\n        \n        running_finetune_loss += loss.item()\n        loop.set_postfix(loss=loss.item())\n    \n    avg_finetune_loss = running_finetune_loss / len(val_loader)\n    finetune_losses.append(avg_finetune_loss)\n    \n    print(f\"Fine-tune Epoch {epoch+1}: loss {avg_finetune_loss:.6f}\")\n    \n    # Save best fine-tuned model\n    if avg_finetune_loss < best_finetune_loss - 1e-7:\n        best_finetune_loss = avg_finetune_loss\n        finetune_patience_counter = 0\n        torch.save(model.state_dict(), '/kaggle/working/finetuned_model.pth')\n        print(f\"  -> New best fine-tuned model saved (loss: {best_finetune_loss:.6f})\")\n    else:\n        finetune_patience_counter += 1\n        if finetune_patience_counter >= finetune_patience:\n            print(f\"  -> Fine-tuning early stopping after {epoch+1} epochs\")\n            break\n\n# Summary of fine-tuning results\nprint(f\"\\nFine-tuning completed!\")\nprint(f\"Initial validation loss: {initial_val_loss:.6f}\")\nprint(f\"Best fine-tuned loss: {best_finetune_loss:.6f}\")\nimprovement = initial_val_loss - best_finetune_loss\nprint(f\"Improvement: {improvement:.6f} ({improvement/initial_val_loss*100:.2f}%)\")\n\n# Load the best fine-tuned model for subsequent evaluation\nmodel.load_state_dict(torch.load('/kaggle/working/finetuned_model.pth', map_location=DEVICE))\nprint(\"Loaded best fine-tuned model for evaluation.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add this as a new cell to visualize the fine-tuning progress\n\n# Visualize Fine-tuning Progress\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n\n# Plot fine-tuning loss progression\nepochs_ft = range(1, len(finetune_losses) + 1)\nax1.plot(epochs_ft, finetune_losses, 'r-', linewidth=2, marker='o', markersize=4)\nax1.axhline(y=initial_val_loss, color='blue', linestyle='--', alpha=0.7, \n            label=f'Initial Val Loss: {initial_val_loss:.6f}')\nax1.axhline(y=best_finetune_loss, color='green', linestyle='--', alpha=0.7,\n            label=f'Best Fine-tuned: {best_finetune_loss:.6f}')\nax1.set_xlabel('Fine-tuning Epoch')\nax1.set_ylabel('Loss')\nax1.set_title('Fine-tuning Progress')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Compare initial vs fine-tuned performance\nmodels = ['Initial\\n(Pre-trained)', 'Fine-tuned\\n(Val Data)']\nlosses = [initial_val_loss, best_finetune_loss]\ncolors = ['blue', 'red']\n\nbars = ax2.bar(models, losses, color=colors, alpha=0.7)\nax2.set_ylabel('Validation Loss')\nax2.set_title('Model Performance Comparison')\nax2.grid(True, alpha=0.3, axis='y')\n\n# Add value labels on bars\nfor bar, loss in zip(bars, losses):\n    height = bar.get_height()\n    ax2.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n             f'{loss:.6f}', ha='center', va='bottom', fontweight='bold')\n\n# Add improvement percentage\nif improvement > 0:\n    improvement_pct = improvement/initial_val_loss*100\n    ax2.text(0.5, max(losses)*0.5, f'Improvement:\\n{improvement:.6f}\\n({improvement_pct:.2f}%)', \n             ha='center', va='center', transform=ax2.transData,\n             bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n\nplt.tight_layout()\nplt.show()\n\n# Print final comparison\nprint(\"\\n\" + \"=\"*50)\nprint(\"FINE-TUNING SUMMARY\")\nprint(\"=\"*50)\nprint(f\"Training Strategy: Pre-trained → Train on 85% → Fine-tune on Validation\")\nprint(f\"Fine-tuning Learning Rate: {finetune_lr}\")\nprint(f\"Fine-tuning Epochs: {len(finetune_losses)}\")\nprint(f\"Initial Model Loss: {initial_val_loss:.6f}\")\nprint(f\"Fine-tuned Model Loss: {best_finetune_loss:.6f}\")\nif improvement > 0:\n    print(f\"Performance Improvement: {improvement:.6f} ({improvement/initial_val_loss*100:.2f}%)\")\nelse:\n    print(f\"Performance Change: {improvement:.6f} (No improvement)\")\nprint(\"=\"*50)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def resize_to_original(arr, original_h, original_w):\n    \"\"\"Resize from 256x256 back to original dimensions\"\"\"\n    return cv2.resize(arr, (original_w, original_h), interpolation=cv2.INTER_LINEAR)\n\n# Load best model\nmodel.load_state_dict(torch.load('/kaggle/working/best_model.pth', map_location=DEVICE))\nmodel.eval()\n\nprint(\"Evaluating on test sequence...\")\ntest_predictions = []\ntest_ground_truths = []\n\nwith torch.no_grad():\n    for xb, yb in tqdm(test_loader, desc=\"Testing\"):\n        xb = xb.to(DEVICE, non_blocking=True)\n        yb = yb.to(DEVICE, non_blocking=True)\n\n        # Get predictions\n        preds = model(xb)\n\n        # Store for later analysis (keep numpy arrays)\n        test_predictions.append(preds.cpu().numpy())\n        test_ground_truths.append(yb.cpu().numpy())\n\n# Concatenate all batches along the batch dimension\nall_test_preds = np.concatenate(test_predictions, axis=0)  # shape: (N, C, H, W)\nall_test_gts = np.concatenate(test_ground_truths, axis=0)  # shape: (N, H, W) or (N, 1, H, W)\n\n# Ensure ground truths have a channel dimension to match predictions\nif all_test_gts.ndim == 3:\n    all_test_gts = all_test_gts[:, np.newaxis, ...]  # (N, 1, H, W)\n\n# Compute overall MSE and MAE across all pixels and samples (correct global averages)\ndiff = all_test_preds - all_test_gts\noverall_test_mse = np.mean(diff ** 2)\noverall_test_mae = np.mean(np.abs(diff))\n\nprint(f\"\\nTotal test samples evaluated: {all_test_preds.shape[0]}\")\nprint(\"\\nOverall Test Results:\")\nprint(f\"Test MSE: {overall_test_mse:.6f}\")\nprint(f\"Test MAE: {overall_test_mae:.6f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test evaluation with detailed metrics\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Load best model\nmodel.load_state_dict(torch.load('/kaggle/working/best_model.pth', map_location=DEVICE))\nmodel.eval()\n\nprint(\"Evaluating on test sequence...\")\ntest_predictions_scaled = []\ntest_ground_truths_scaled = []\ntest_indices = []\n\nwith torch.no_grad():\n    for i, (xb, yb) in enumerate(tqdm(test_loader, desc=\"Testing\")):\n        xb = xb.to(DEVICE, non_blocking=True)\n        yb = yb.to(DEVICE, non_blocking=True)\n        \n        # Get predictions\n        preds = model(xb)\n        \n        # Store for later analysis (keep numpy arrays on scaled data)\n        test_predictions_scaled.append(preds.cpu().numpy())\n        test_ground_truths_scaled.append(yb.cpu().numpy())\n        test_indices.extend([val_size + LOOKBACK_DAYS + i*4 + j for j in range(len(xb))])\n\n# Concatenate all batches\nall_test_preds_scaled = np.concatenate(test_predictions_scaled, axis=0)  # (N, 1, 256, 256)\nall_test_gts_scaled = np.concatenate(test_ground_truths_scaled, axis=0)    # (N, 256, 256)\n\n# Ensure ground truths have channel dimension\nif all_test_gts_scaled.ndim == 3:\n    all_test_gts_scaled = all_test_gts_scaled[:, np.newaxis, ...]  # (N, 1, 256, 256)\n\nprint(f\"Test predictions shape: {all_test_preds_scaled.shape}\")\nprint(f\"Test ground truths shape: {all_test_gts_scaled.shape}\")\n\n# Calculate metrics on scaled data\ndiff_scaled = all_test_preds_scaled - all_test_gts_scaled\nmse_scaled = np.mean(diff_scaled ** 2)\nmae_scaled = np.mean(np.abs(diff_scaled))\n\nprint(f\"\\nScaled Data Metrics:\")\nprint(f\"MSE (scaled): {mse_scaled:.6f}\")\nprint(f\"MAE (scaled): {mae_scaled:.6f}\")\n\n# Resize back to original dimensions and inverse transform to original scale\ntest_predictions_original = []\ntest_ground_truths_original = []\n\nfor i in range(len(all_test_preds_scaled)):\n    # Resize from 256x256 back to original HxW\n    pred_resized = resize_to_original(all_test_preds_scaled[i, 0], H, W)\n    gt_resized = resize_to_original(all_test_gts_scaled[i, 0], H, W)\n    \n    # Flatten to 1D for inverse transform\n    pred_flat = pred_resized.flatten().reshape(1, -1)\n    gt_flat = gt_resized.flatten().reshape(1, -1)\n    \n    # Inverse transform using the scaler\n    pred_original = scaler.inverse_transform(pred_flat).flatten()\n    gt_original = scaler.inverse_transform(gt_flat).flatten()\n    \n    test_predictions_original.append(pred_original.reshape(H, W))\n    test_ground_truths_original.append(gt_original.reshape(H, W))\n\n# Convert to numpy arrays\ntest_predictions_original = np.array(test_predictions_original)\ntest_ground_truths_original = np.array(test_ground_truths_original)\n\nprint(f\"Original scale predictions shape: {test_predictions_original.shape}\")\nprint(f\"Original scale ground truths shape: {test_ground_truths_original.shape}\")\n\n# Calculate metrics on original scale\ndiff_original = test_predictions_original - test_ground_truths_original\nmse_original = np.mean(diff_original ** 2)\nmae_original = np.mean(np.abs(diff_original))\n\nprint(f\"\\nOriginal Scale Metrics:\")\nprint(f\"MSE (original): {mse_original:.6f}\")\nprint(f\"MAE (original): {mae_original:.6f}\")\nprint(f\"RMSE (original): {np.sqrt(mse_original):.6f}\")\n\nprint(f\"\\nTotal test samples evaluated: {len(test_predictions_original)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Forecast next 7 days after the data range\nprint(\"Generating 7-day forecast after the data range...\")\n\n# Get the last LOOKBACK_DAYS from the dataset for forecasting\nlast_sequence_files = meta_df['file'].iloc[-LOOKBACK_DAYS:].tolist()\nprint(\"days: \",last_sequence_files)\nforecast_input = np.stack([np.load(f) for f in last_sequence_files], axis=0)  # Shape: (7, 256, 256)\nforecast_input_tensor = torch.from_numpy(forecast_input).float().unsqueeze(0).to(DEVICE)\n\nprint(f\"Forecast input shape: {forecast_input_tensor.shape}\")\n\n# Generate 7-day forecast\nforecast_predictions_scaled = []\ncurrent_input = forecast_input_tensor.clone()\n\nwith torch.no_grad():\n    for day in range(7):\n        # Predict next day\n        next_day_pred = model(current_input)\n        forecast_predictions_scaled.append(next_day_pred.cpu().numpy()[0, 0])  # Remove batch and channel dims\n        \n        # Update input sequence: remove oldest day, add predicted day\n        next_day_pred_expanded = next_day_pred.squeeze(0)  # Remove batch dim, keep channel dim\n        current_input = torch.cat([current_input[:, 1:], next_day_pred_expanded.unsqueeze(0)], dim=1)\n\nforecast_predictions_scaled = np.array(forecast_predictions_scaled)  # Shape: (7, 256, 256)\nprint(f\"Forecast predictions shape: {forecast_predictions_scaled.shape}\")\n\n# Convert forecasts to original scale\nforecast_predictions_original = []\n\nfor i in range(7):\n    # Resize from 256x256 back to original HxW\n    forecast_resized = resize_to_original(forecast_predictions_scaled[i], H, W)\n    \n    # Flatten and inverse transform\n    forecast_flat = forecast_resized.flatten().reshape(1, -1)\n    forecast_original = scaler.inverse_transform(forecast_flat).flatten()\n    forecast_predictions_original.append(forecast_original.reshape(H, W))\n\nforecast_predictions_original = np.array(forecast_predictions_original)\n\n# Generate forecast dates - Start from the user-defined LAST_DATE variable\nforecast_dates = pd.date_range(start=LAST_DATE + pd.Timedelta(days=1), periods=7, freq='D')\n\nprint(f\"Starting forecast from: {LAST_DATE}\")\nprint(f\"Forecast generated for dates: {forecast_dates[0].date()} to {forecast_dates[-1].date()}\")\nprint(f\"Forecast shape (original scale): {forecast_predictions_original.shape}\")\n\n# Summary statistics for forecast\nprint(f\"\\nForecast Summary (original scale):\")\nprint(f\"Min value: {forecast_predictions_original.min():.6f}\")\nprint(f\"Max value: {forecast_predictions_original.max():.6f}\")\nprint(f\"Mean value: {forecast_predictions_original.mean():.6f}\")\nprint(f\"Std value: {forecast_predictions_original.std():.6f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Test Results Comparison\nplt.rcParams['figure.figsize'] = (20, 15)\n\n# Calculate proper aspect ratio and figure dimensions based on H, W\naspect_ratio = W / H  # 310 / 118 ≈ 2.63\nimg_height = 4  # Base height for individual images\nimg_width = img_height * aspect_ratio  # Width scaled by aspect ratio\n\n# Get the last 3 days from the user-defined LAST_DATE variable for display\ntest_display_dates = pd.date_range(end=LAST_DATE, periods=3, freq='D')\n\nfig, axes = plt.subplots(3, 4, figsize=(20, 12))\nfig.suptitle('Test Predictions vs Ground Truth Comparison', fontsize=16)\n\n# Show first 3 test samples with their corresponding dates\nfor i in range(min(3, len(test_predictions_original))):\n    sample_date = test_display_dates[i].strftime('%Y-%m-%d')\n    \n    # Ground truth\n    im1 = axes[i, 0].imshow(test_ground_truths_original[i], cmap='viridis', aspect='equal', origin='lower')\n    axes[i, 0].set_title(f'Ground Truth - {sample_date}')\n    axes[i, 0].axis('off')\n    plt.colorbar(im1, ax=axes[i, 0], fraction=0.046, pad=0.04)\n    \n    # Prediction\n    im2 = axes[i, 1].imshow(test_predictions_original[i], cmap='viridis', aspect='equal', origin='lower')\n    axes[i, 1].set_title(f'Prediction - {sample_date}')\n    axes[i, 1].axis('off')\n    plt.colorbar(im2, ax=axes[i, 1], fraction=0.046, pad=0.04)\n    \n    # Absolute Error (MAE Map)\n    abs_error_img = np.abs(test_predictions_original[i] - test_ground_truths_original[i])\n    im3 = axes[i, 2].imshow(abs_error_img, cmap='Reds', aspect='equal', origin='lower')\n    axes[i, 2].set_title(f'Absolute Error - {sample_date}')\n    axes[i, 2].axis('off')\n    plt.colorbar(im3, ax=axes[i, 2], fraction=0.046, pad=0.04)\n    \n    # Scatter plot\n    axes[i, 3].scatter(test_ground_truths_original[i].flatten(), \n                       test_predictions_original[i].flatten(), \n                       alpha=0.5, s=1)\n    axes[i, 3].plot([test_ground_truths_original[i].min(), test_ground_truths_original[i].max()], \n                    [test_ground_truths_original[i].min(), test_ground_truths_original[i].max()], \n                    'r--', linewidth=2)\n    axes[i, 3].set_xlabel('Ground Truth')\n    axes[i, 3].set_ylabel('Prediction')\n    axes[i, 3].set_title(f'Correlation - {sample_date}')\n    \n    # Calculate correlation\n    corr = np.corrcoef(test_ground_truths_original[i].flatten(), \n                       test_predictions_original[i].flatten())[0, 1]\n    axes[i, 3].text(0.05, 0.95, f'R = {corr:.3f}', transform=axes[i, 3].transAxes, \n                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nplt.tight_layout()\nplt.show()\n\n# Print date information for clarity\nprint(f\"\\nTest Results Display Information:\")\nprint(f\"Last day (LAST_DATE): {LAST_DATE.strftime('%Y-%m-%d')}\")\nprint(f\"Test samples shown for dates: {test_display_dates[0].strftime('%Y-%m-%d')} to {test_display_dates[-1].strftime('%Y-%m-%d')}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. Forecast Visualization - Individual Days\n# Calculate proper figure size to maintain aspect ratio\nsingle_img_width = 6 * aspect_ratio  # Width for single image\nsingle_img_height = 6  # Height for single image\n\nfig, axes = plt.subplots(2, 4, figsize=(24, 2 * single_img_height))\nfig.suptitle('7-Day Forecast Results', fontsize=16)\n\n# Show all 7 forecast days\nfor i in range(7):\n    row = i // 4\n    col = i % 4\n    \n    im = axes[row, col].imshow(forecast_predictions_original[i], cmap='viridis', aspect='equal', origin='lower')\n    axes[row, col].set_title(f'Day +{i+1}: {forecast_dates[i].strftime(\"%Y-%m-%d\")}', fontsize=12)\n    axes[row, col].axis('off')\n    plt.colorbar(im, ax=axes[row, col], fraction=0.046, pad=0.04)\n\n# Remove the empty subplot (8th position)\nfig.delaxes(axes[1, 3])\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4. Time Series Analysis - Historical Data + Forecasts\nfig, axes = plt.subplots(2, 1, figsize=(15, 10))\n\n# Historical data (last 30 days) + forecasts\nhist_days = 30\nif len(meta_df) >= hist_days:\n    hist_start_idx = len(meta_df) - hist_days\n    hist_dates = pd.to_datetime(meta_df['datetime'].iloc[hist_start_idx:])\n    hist_means = []\n    \n    for i in range(hist_start_idx, len(meta_df)):\n        data_scaled = np.load(meta_df['file'].iloc[i])  # Shape: (256, 256)\n        # Resize back to original dimensions\n        data_resized = resize_to_original(data_scaled, H, W)\n        data_original = scaler.inverse_transform(data_resized.flatten().reshape(1, -1)).flatten()\n        hist_means.append(data_original.reshape(H, W).mean())\n    \n    # Plot historical + forecast means\n    all_dates = list(hist_dates) + list(forecast_dates)\n    all_means = hist_means + [day.mean() for day in forecast_predictions_original]\n    \n    axes[0].plot(hist_dates, hist_means, 'b-', linewidth=2, label='Historical', marker='o', markersize=4)\n    axes[0].plot(forecast_dates, [day.mean() for day in forecast_predictions_original], \n                'r-', linewidth=2, label='Forecast', marker='s', markersize=4)\n    axes[0].axvline(x=hist_dates.iloc[-1], color='gray', linestyle='--', alpha=0.7)\n    axes[0].set_xlabel('Date')\n    axes[0].set_ylabel('Mean NO2 Value')\n    axes[0].set_title('Time Series: Historical Data + 7-Day Forecast')\n    axes[0].legend()\n    axes[0].grid(True, alpha=0.3)\n    axes[0].tick_params(axis='x', rotation=45)\n\n# Forecast uncertainty (std across spatial dimensions)\nforecast_stds = [day.std() for day in forecast_predictions_original]\nforecast_means = [day.mean() for day in forecast_predictions_original]\n\naxes[1].plot(forecast_dates, forecast_means, 'r-', linewidth=2, marker='s', markersize=6, label='Mean')\naxes[1].fill_between(forecast_dates, \n                     np.array(forecast_means) - np.array(forecast_stds),\n                     np.array(forecast_means) + np.array(forecast_stds),\n                     alpha=0.3, label='±1 Std')\naxes[1].set_xlabel('Date')\naxes[1].set_ylabel('NO2 Value')\naxes[1].set_title('7-Day Forecast with Spatial Uncertainty')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\naxes[1].tick_params(axis='x', rotation=45)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 5. Additional Forecast Visualizations - Side by Side Comparison\n# Create a comprehensive comparison view\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Show first, middle, and last forecast day\nforecast_indices = [0, 3, 6]  # Days 1, 4, and 7\nforecast_labels = ['Day +1', 'Day +4', 'Day +7']\n\nfor idx, (f_idx, label) in enumerate(zip(forecast_indices, forecast_labels)):\n    im = axes[idx].imshow(forecast_predictions_original[f_idx], cmap='viridis', aspect='equal', origin='lower')\n    axes[idx].set_title(f'{label}: {forecast_dates[f_idx].strftime(\"%Y-%m-%d\")}', fontsize=12)\n    axes[idx].axis('off')\n    plt.colorbar(im, ax=axes[idx], fraction=0.046, pad=0.04)\n\nplt.suptitle('Forecast Evolution: Selected Days', fontsize=14)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 6. Final Results Summary\nprint(\"\\n\" + \"=\"*60)\nprint(\"FINAL RESULTS SUMMARY\")\nprint(\"=\"*60)\nprint(f\"Data Dimensions: {H} × {W} pixels\")\nprint(f\"Aspect Ratio: {aspect_ratio:.2f}\")\nprint(f\"\\nTest Set Performance:\")\nprint(f\"  • MSE (original scale): {mse_original:.6f}\")\nprint(f\"  • MAE (original scale): {mae_original:.6f}\")\nprint(f\"  • RMSE (original scale): {np.sqrt(mse_original):.6f}\")\nprint(f\"  • Correlation: {overall_corr:.3f}\")\nprint(f\"  • Number of test samples: {len(test_predictions_original)}\")\nprint(f\"\\nForecast Summary:\")\nprint(f\"  • Forecast period: {forecast_dates[0].strftime('%Y-%m-%d')} to {forecast_dates[-1].strftime('%Y-%m-%d')}\")\nprint(f\"  • Mean forecast value: {forecast_predictions_original.mean():.6f}\")\nprint(f\"  • Forecast std: {forecast_predictions_original.std():.6f}\")\nprint(f\"  • Forecast range: [{forecast_predictions_original.min():.6f}, {forecast_predictions_original.max():.6f}]\")\nprint(f\"\\nSpatial Statistics:\")\nprint(f\"  • Test correlation per sample: Min={np.array([np.corrcoef(test_ground_truths_original[i].flatten(), test_predictions_original[i].flatten())[0,1] for i in range(len(test_predictions_original))]).min():.3f}, Max={np.array([np.corrcoef(test_ground_truths_original[i].flatten(), test_predictions_original[i].flatten())[0,1] for i in range(len(test_predictions_original))]).max():.3f}\")\nprint(f\"  • Mean squared error per sample: Min={sample_mse.min():.6f}, Max={sample_mse.max():.6f}\")\nprint(\"=\"*60)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save 7-day forecast in original data format (datetime + flattened pixel columns)\nprint(\"Preparing forecast data in original format...\")\n\n# Create forecast dataframe\nforecast_data_rows = []\n\nfor i, forecast_date in enumerate(forecast_dates):\n    # Get the 2D forecast for this day (shape: H, W)\n    forecast_2d = forecast_predictions_original[i]  # Shape: (118, 310)\n    \n    # Flatten to 1D (same as original data format)\n    forecast_flattened = forecast_2d.flatten()  # Shape: (36580,) = 118*310\n    \n    # Create row: [datetime, pixel_0, pixel_1, pixel_2, ...]\n    row = [forecast_date] + forecast_flattened.tolist()\n    forecast_data_rows.append(row)\n\n# Create column names (same as original data format)\nnum_pixels = H * W  # 118 * 310 = 36580\ncolumn_names = ['datetime'] + [str(i) for i in range(num_pixels)]\n\n# Create DataFrame\nforecast_df = pd.DataFrame(forecast_data_rows, columns=column_names)\n\n# Display info about the forecast dataframe\nprint(f\"Forecast DataFrame shape: {forecast_df.shape}\")\nprint(f\"Columns: datetime + {num_pixels} pixel columns (0 to {num_pixels-1})\")\nprint(f\"Date range: {forecast_df['datetime'].iloc[0].strftime('%Y-%m-%d')} to {forecast_df['datetime'].iloc[-1].strftime('%Y-%m-%d')}\")\n\n# Show first few rows and columns\nprint(\"\\nFirst 3 rows and first 10 columns:\")\nprint(forecast_df.iloc[:3, :10])\n\n# Save to CSV\n#forecast_output_path = '/kaggle/working/no2_forecast_7days.csv'\n#forecast_output_path = \"/kaggle/working/o3_forecast_7days.csv\"\nforecast_output_path = \"/kaggle/working/hcho_forecast_7days.csv\"\n\nforecast_df.to_csv(forecast_output_path, index=False)\nprint(f\"\\nForecast saved to: {forecast_output_path}\")\n\n# Verify the saved file\nprint(f\"Saved file shape: {pd.read_csv(forecast_output_path).shape}\")\nprint(\"File successfully saved in original data format!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Verify forecast data format and show sample statistics\nprint(\"=\"*60)\nprint(\"FORECAST DATA VERIFICATION\")\nprint(\"=\"*60)\n\n# Load the saved forecast to verify\nsaved_forecast = pd.read_csv(forecast_output_path)\n\nprint(f\"Saved forecast shape: {saved_forecast.shape}\")\nprint(f\"Expected shape: (7, {num_pixels + 1})\")  # 7 days, datetime + pixels\nprint(f\"Shape matches expected: {saved_forecast.shape == (7, num_pixels + 1)}\")\n\nprint(f\"\\nColumn names:\")\nprint(f\"First column (datetime): '{saved_forecast.columns[0]}'\")\nprint(f\"Pixel columns: {saved_forecast.columns[1]} to {saved_forecast.columns[-1]}\")\nprint(f\"Total pixel columns: {len(saved_forecast.columns) - 1}\")\n\nprint(f\"\\nData sample (first day, first 10 pixels):\")\nfirst_day = saved_forecast.iloc[0]\nprint(f\"Date: {first_day['datetime']}\")\nprint(\"First 10 pixel values:\", first_day[['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']].values)\n\nprint(f\"\\nForecast statistics across all days and pixels:\")\npixel_columns = saved_forecast.columns[1:]  # All columns except datetime\nforecast_values = saved_forecast[pixel_columns].values.flatten()\nprint(f\"Min value: {forecast_values.min():.6f}\")\nprint(f\"Max value: {forecast_values.max():.6f}\")\nprint(f\"Mean value: {forecast_values.mean():.6f}\")\nprint(f\"Std value: {forecast_values.std():.6f}\")\n\nprint(f\"\\nDaily statistics:\")\nfor i, row in saved_forecast.iterrows():\n    day_values = row[pixel_columns].values\n    print(f\"Day {i+1} ({row['datetime']}): mean={day_values.mean():.6f}, std={day_values.std():.6f}\")\n\nprint(\"=\"*60)\nprint(\"FORECAST DATA READY FOR USE!\")\nprint(\"=\"*60)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#df2=pd.read_csv(\"/kaggle/working/no2_forecast_7days.csv\")\n#df2=pd.read_csv(\"/kaggle/working/hcho_forecast_7days.csv\")\ndf2=pd.read_csv(\"/kaggle/working/o3_forecast_7days.csv\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}